{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3960cd00-6671-446d-849b-4b5de0c75d3d",
   "metadata": {},
   "source": [
    "## Using TensorRT-LLM to Run Sovereign Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ffb2dc-1d65-409d-9e99-db12b430fdf0",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb181496-1e38-48f4-9480-6f420309c3cf",
   "metadata": {},
   "source": [
    "Please make sure that you complete the following steps before launching this notebook on a Linux machine. These steps walk through running the required docker container and installing the libraries required for TensorRT-LLM. These steps are also highlight in the installation guide\n",
    "\n",
    "docker run --rm -it --ipc=host --net=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all --volume ${PWD}:/workspace --workdir /workspace nvidia/cuda:12.4.1-devel-ubuntu22.04\n",
    "\n",
    "apt-get update && apt-get -y install python3.10 python3-pip openmpi-bin libopenmpi-dev git git-lfs\n",
    "\n",
    "pip install jupyterlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be146e2d-ea65-45f4-bc9d-692a751990be",
   "metadata": {},
   "source": [
    "### Install TensorRT-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b9187-a839-47a3-b426-d932ffcd7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipywidgets\n",
    "!pip install tensorrt_llm -U -q --extra-index-url https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0953c74b-1e9d-4820-9bac-84a19985c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/examples/run.py -P .\n",
    "!wget https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/examples/utils.py -P .\n",
    "!wget https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/examples/llama/convert_checkpoint.py -P ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92739fd-271b-4c2b-a1e9-6119ed1cf0a2",
   "metadata": {},
   "source": [
    "### Swallow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb2841-2ca1-4a51-9b4e-379b15f4eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb8a762-12cc-490b-897d-555c4cefccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone HF model repo\n",
    "\n",
    "!git clone https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b11bcd67-632f-4bc7-8732-9c2671a52705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.11.0\n",
      "0.11.0\n",
      "Total time of converting checkpoints: 00:05:04\n"
     ]
    }
   ],
   "source": [
    "# Convert HF checkpoint to TRT-LLM format using an 8-GPU single node.\n",
    "\n",
    "!python3 convert_checkpoint.py --model_dir ./Swallow-70b-instruct-v0.1 \\\n",
    "                         --output_dir ./tllm_checkpoint_8gpu_tp8_swallow \\\n",
    "                         --dtype float16 --tp_size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c05e0-b62f-4df0-be20-189b33ad1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build TRT engine using an 8-GPU single node.\n",
    "\n",
    "!trtllm-build --checkpoint_dir ./tllm_checkpoint_8gpu_tp8_swallow \\\n",
    "             --output_dir ./swallow/70B/trt_engines/fp16/8-gpu/ \\\n",
    "             --gemm_plugin auto \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58296569-3ce3-4565-892b-cfb06d5d4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "\n",
    "!python3 run.py --engine_dir ./swallow/70B/trt_engines/fp16/8-gpu \\\n",
    "                 --max_output_len 500 \\\n",
    "                 --tokenizer_dir ./Swallow-70b-instruct-v0.1 \\\n",
    "                 --input_text \"your prompt here\" \\\n",
    "                 --use_py_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34528c1-5e96-4145-8880-4c378198f0d5",
   "metadata": {},
   "source": [
    "### Tame Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de45e9e0-eb20-41c8-a39d-4547618328aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone HF model repo\n",
    "\n",
    "!git clone https://huggingface.co/yentinglin/Llama-3-Taiwan-70B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39e80a-d64a-41f4-adbe-8fe26ede60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert HF checkpoint to TRT-LLM format using an 8-GPU single node.\n",
    "\n",
    "!python3 convert_checkpoint.py --model_dir ./Llama-3-Taiwan-70B-Instruct \\\n",
    "                         --output_dir ./tllm_checkpoint_8gpu_tp8_tame \\\n",
    "                         --dtype float16 --tp_size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071d07c-a8c0-4189-9217-d85562b1d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build TRT engine using an 8-GPU single node.\n",
    "\n",
    "!trtllm-build --checkpoint_dir ./tllm_checkpoint_8gpu_tp8_tame \\\n",
    "             --output_dir ./tame/70B/trt_engines/fp16/8-gpu/ \\\n",
    "             --gemm_plugin auto \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d11b7-7d9a-4536-aba3-6efee112bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "\n",
    "!python3 run.py --engine_dir ./tame/70B/trt_engines/fp16/8-gpu \\\n",
    "                 --max_output_len 500 \\\n",
    "                 --tokenizer_dir ./Llama-3-Taiwan-70B-Instruct \\\n",
    "                 --input_text \"your prompt here\" \\\n",
    "                 --use_py_session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
