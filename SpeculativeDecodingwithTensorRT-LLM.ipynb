{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT-LLM Speculative Decoding \n",
    "### Boost AI Inference Throughout by Up to 3.6x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you'll learn how to use NVIDIA's [TensorRT-LLM](https://developer.nvidia.com/tensorrt#section-inference-for-llms) to boost inference throughput using speculative decoding. TensorRT-LLM is an open-source library that provides blazing-fast inference support for numerous popular large language models [(LLMs)](https://www.nvidia.com/en-us/glossary/large-language-models/) on NVIDIA GPUs. By adding support for speculative decoding on single GPU and single node multi-GPU, the library further expands its supported optimizations to provide best performance for generative AI applications. \n",
    "\n",
    "Speculative decoding, also referred to as [speculative sampling](https://arxiv.org/abs/2302.01318), works by paying a small additional computation cost to speculatively generate the next several tokens, and then using the target model to perform a built-in verification step to ensure the quality of output generation while giving a throughput boost. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 16px;\">\n",
    "    <p><b>Figure 1. Speculating decoding algorithm</b></p>\n",
    "    <img src=\"Draft Target Speculative Decoding.jpg\" width=\"600\" height=\"500\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 16px;\">\n",
    "    <p><b>Figure 2. Throughput Speedups with Llama 3.1 405B Target and Different Draft Models</b></p>\n",
    "    <img src=\"Speculative decoding throughput speedups.jpg\" width=\"800\" height=\"400\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=1200, height=500 controls>\n",
       "    <source src=\"./speculative_decoding_trt-llm.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=1200, height=500 controls>\n",
    "    <source src=\"./speculative_decoding_trt-llm.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; font-size: 16px; width=1200; height=500\">\n",
    "    <div style=\"width: 15%\"></div>\n",
    "    <div style=\"width: 30%; text-align:right; font-weight:bold; color:blue\">\n",
    "        Speculative decoding off \n",
    "    </div>\n",
    "    <div style=\"width: 5%\"></div>\n",
    "    <div style=\"width: 40%; text-align:center; font-weight:bold; color:blue\">\n",
    "        Speculative decoding on\n",
    "    </div>\n",
    "    <div style=\"width: 10%\"></div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to run speculative decoding in TensorRT-LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure that you complete the following steps before launching this notebook on a Linux machine. These steps walk through running the required docker container and installing the libraries required for TensorRT-LLM. These steps are also highlight in the [installation guide](https://nvidia.github.io/TensorRT-LLM/installation/linux.html)\n",
    "\n",
    "- docker run --rm -it --ipc=host --net=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all --volume ${PWD}:/workspace --workdir /workspace nvidia/cuda:12.4.1-devel-ubuntu22.04\n",
    "\n",
    "- apt-get update && apt-get -y install python3.10 python3-pip openmpi-bin libopenmpi-dev git git-lfs\n",
    "\n",
    "- pip install jupyterlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install TensorRT-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ipywidgets\n",
    "!pip install tensorrt_llm -U -q --extra-index-url https://pypi.nvidia.com\n",
    "\n",
    "!wget https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/examples/run.py -P .\n",
    "!wget https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/examples/utils.py -P .\n",
    "!wget https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/examples/quantization/quantize.py -P ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Download draft and target models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download target model\n",
    "!git clone https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct\n",
    "\n",
    "# Download draft models\n",
    "!git clone https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
    "!git clone https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
    "!git clone https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FP8 checkpoints\n",
    "\n",
    "!python3 quantization/quantize.py --model_dir <path to draft model repo> --dtype float16 --qformat fp8 --kv_cache_dtype fp8 \n",
    "--output_dir /ckpt-draft --calib_size 512 --tp_size 4\n",
    "\n",
    "!python3 quantization/quantize.py \\\n",
    "    --model_dir=<path to target model repo> \\\n",
    "    --output_dir=./ckpt-target-405b \\\n",
    "    --dtype=float16 --qformat fp8 --kv_cache_dtype fp8 \\\n",
    "    --calib_size 512 --tp_size 4 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build draft and target engines\n",
    "# Important flags for the engine build process:\n",
    "# --use_paged_context_fmha=enable must be specified since we need KVcache reuse for the draft/target model.\n",
    "\n",
    "# --speculative_decoding_mode=draft_tokens_external and --max_draft_len must be specified for target model.\n",
    "\n",
    "!trtllm-build \\\n",
    "    --checkpoint_dir ./ckpt-draft \\\n",
    "    --output_dir=./draft-engine \\\n",
    "    --gpt_attention_plugin float16 \\\n",
    "    --workers 4 \\\n",
    "    --gemm_plugin=fp8 \\\n",
    "    --reduce_fusion disable \\\n",
    "    --use_paged_context_fmha=enable \\\n",
    "    --use_fused_mlp enable \\\n",
    "    --multiple_profiles enable \\\n",
    "    --max_batch_size=32 \\\n",
    "    --max_num_tokens=8192 \\\n",
    "    --max_seq_len=131072\n",
    "\n",
    "!trtllm-build \\\n",
    "    --checkpoint_dir=./ckpt-target-405b \\\n",
    "    --output_dir=./target-engine \\\n",
    "    --gpt_attention_plugin float16 \\\n",
    "    --workers 4 \\\n",
    "    --gemm_plugin=fp8 \\\n",
    "    --use_paged_context_fmha=enable \\\n",
    "    --use_fused_mlp enable \\\n",
    "    --multiple_profiles enable \\\n",
    "    --max_batch_size=32 \\\n",
    "    --max_num_tokens=8192 \\\n",
    "    --max_seq_len=131072 \\\n",
    "    --low_latency_gemm_plugin fp8 \\\n",
    "    --speculative_decoding_mode=draft_tokens_external \\\n",
    "    --max_draft_len 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Run speculative decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run decoding\n",
    "\n",
    "# Important flags to set during the run process:\n",
    "#--draft_engine_dir and --engine_dir must be specified for the draft and target engines.\n",
    "\n",
    "#--draft_target_model_config is corresponding to the configuration of Draft-Target-Model. As an example, [4,[0],[1],False] means draft_len=4, device of draft model is GPU0, device of target model is GPU1, and use tokens rather than logits to accept.\n",
    "\n",
    "# Only CPP session (using executor as low-level API) is supported, while Python session (--use_py_session) is not supported.\n",
    "\n",
    "# Run with 405B target model\n",
    "\n",
    "!mpirun -n 8 --allow-run-as-root python3 ./run.py \\\n",
    "    --tokenizer_dir <path to draft model repo> \\\n",
    "    --draft_engine_dir ./draft-engine \\\n",
    "    --engine_dir ./target-engine \\     \n",
    "    --draft_target_model_config = \"[10,[0,1,2,3,4,5,6,7],[0,1,2,3,4,5,6,7], False]\" \\\n",
    "    --kv_cache_free_gpu_memory_fraction=0.35 \\\n",
    "    --max_output_len=1024 \\\n",
    "    --kv_cache_enable_block_reuse \\\n",
    "    --input_text=\"Implement a program to find the common elements in two arrays without using any extra data structures.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
